# -*- coding: utf-8 -*-
"""SMARTSDLC AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jn0pi6f0bdbIUe2fLoWZI0pUw4HEx0w-
"""

!pip install transformers torch gradio

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
  tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=512):
  inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)

  if torch.cuda.is_available():
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature = 0.7,
        do_sample = True,
        pad_token_id = tokenizer.eos_token_id
    )
  response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  response = response.replace(prompt,"").strip()
  return response

def concept_explanation(concept):
  prompt = f"Explain the concept of {concept} in detail with example"
  return generate_response(prompt, max_length=800)

def quiz_generator(concept):
  prompt = f"Generate 5 quiz question about {concept} with different quies types (multiple choice, true/false, short answer):, Give me the answer at the end
